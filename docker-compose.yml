services:
  postgres:
    image: postgres:14
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - ./database/:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d db -U airflow || exit 1"]
      interval: 15s
      retries: 3
    ports:
      - "5433:5432"
    restart: always


  scheduler:
    build: .
    command: airflow scheduler
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --local || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - postgres
    environment:
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      #set the dbt directory
      - DBT_DIR=/app/airflow/dbt_linkedin_etl_project
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE:-SNOWFLAKE_LEARNING_WH}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE:-linkedin_db}
      - SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA:-linkedin_raw}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./dags:/app/airflow/dags
      - ./data:/app/airflow/data
      - ./logs:/app/airflow/logs
      #added dbt volumes
      - ./dbt_linkedin_etl_project:/app/airflow/dbt_linkedin_etl_project



  webserver:
    build: .
    entrypoint: /app/airflow/scripts/entrypoint.sh
    command: ["webserver",
              "--hostname",
              "0.0.0.0",
              "--gunicorn-timeout",
              "120",
              "--workers",
              "5"]
    restart: on-failure
    depends_on:
      - postgres
      - scheduler
    environment:
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_USER=${POSTGRES_USER}
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE:-SNOWFLAKE_LEARNING_WH}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE:-linkedin_db}
      - SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA:-linkedin_raw}
      #set the dbt directory
      - DBT_DIR=/app/airflow/dbt_linkedin_etl_project
    volumes:
      - ./dags:/app/airflow/dags
      - ./data:/app/airflow/data
      - ./logs:/app/airflow/logs
      #added dbt volumes
      - ./dbt_linkedin_etl_project:/app/airflow/dbt_linkedin_etl_project
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "airflow", "db", "check"]
      interval: 30s
      timeout: 30s
      retries: 3
